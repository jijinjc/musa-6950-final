{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eiB7WGgGvTwr",
      "metadata": {
        "id": "eiB7WGgGvTwr"
      },
      "source": [
        "### MaskRCNN in detecting objects from irregular sources\n",
        "\n",
        "This notebook will be using a simple MaskRCNN model to try and identify objects in urban environments from YouTube videos. The MaskRCNN model used in this project uses a ResNet-50 FPN V2 backbone from torchvision library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562e8e77",
      "metadata": {
        "id": "562e8e77"
      },
      "source": [
        "### Setting Up Your Python Environment\n",
        "\n",
        "This section is mainly used for importing packagesnd properly setting up the environment for use in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A8XYrxWnKQeC",
      "metadata": {
        "id": "A8XYrxWnKQeC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ku3YCSo8anC",
      "metadata": {
        "id": "7ku3YCSo8anC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install matplotlib pandas pillow torchtnt==0.2.0 tqdm tabulate\n",
        "!pip install distinctipy\n",
        "!pip install cjm_pandas_utils cjm_psl_utils cjm_pil_utils cjm_pytorch_utils cjm_torchvision_tfms\n",
        "!pip install torchtnt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f835e2b",
      "metadata": {
        "id": "3f835e2b"
      },
      "source": [
        "### Importing the Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4c9e24",
      "metadata": {
        "id": "af4c9e24"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from functools import partial\n",
        "from glob import glob\n",
        "import json\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "from typing import Any, Dict, Optional\n",
        "from cjm_psl_utils.core import download_file, file_extract, get_source_code\n",
        "from cjm_pil_utils.core import resize_img, get_img_files, stack_imgs\n",
        "from cjm_pytorch_utils.core import pil_to_tensor, tensor_to_pil, get_torch_device, set_seed, denorm_img_tensor, move_data_to_device\n",
        "from cjm_pandas_utils.core import markdown_to_pandas, convert_to_numeric, convert_to_string\n",
        "from cjm_torchvision_tfms.core import ResizeMax, PadSquare, CustomRandomIoUCrop\n",
        "from distinctipy import distinctipy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', None)  # Do not truncate the contents of cells in the DataFrame\n",
        "pd.set_option('display.max_rows', None)  # Display all rows in the DataFrame\n",
        "pd.set_option('display.max_columns', None)  # Display all columns in the DataFrame\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "from torch.amp import autocast\n",
        "from torch.cuda.amp import GradScaler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtnt.utils import get_module_summary\n",
        "import torchvision\n",
        "torchvision.disable_beta_transforms_warning()\n",
        "from torchvision.tv_tensors import BoundingBoxes, Mask\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
        "import torchvision.transforms.v2  as transforms\n",
        "from torchvision.transforms.v2 import functional as TF\n",
        "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN\n",
        "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision.transforms.v2 import Resize, SanitizeBoundingBoxes\n",
        "from torchvision.transforms.v2 import Compose\n",
        "import torchvision.transforms.v2  as transforms\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YmRolbrQ8SmF",
      "metadata": {
        "id": "YmRolbrQ8SmF"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float32\n",
        "device, dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62EPNUZOapuU",
      "metadata": {
        "id": "62EPNUZOapuU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-xthmdiwwMaP",
      "metadata": {
        "id": "-xthmdiwwMaP"
      },
      "source": [
        "### Preapare the training and testing data\n",
        "\n",
        "We will first test on one of the frames we have labelled. This will require us to get annotations, label the annotations correctly, and display them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WEgxx6QJGzrI",
      "metadata": {
        "id": "WEgxx6QJGzrI"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ToFFFMMZKhyJ",
      "metadata": {
        "id": "ToFFFMMZKhyJ"
      },
      "outputs": [],
      "source": [
        "#dataset_path = Path('/content/drive/MyDrive/Lab8/car_training_images_100_augmented')\n",
        "\n",
        "dataset_path = Path('/content/drive/MyDrive/AI Final/all_chosen_frames')\n",
        "\n",
        "# Get a list of image files in the dataset\n",
        "img_file_paths = get_img_files(dataset_path)\n",
        "\n",
        "# Get a list of JSON files in the dataset\n",
        "annotation_file_paths = list(dataset_path.glob('*.json'))\n",
        "\n",
        "# Create a dictionary that maps file names to file paths\n",
        "img_dict = {file.stem : file for file in img_file_paths}\n",
        "\n",
        "# Print the number of image files\n",
        "print(f\"Number of Images: {len(img_dict)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GD4L7pjf0xHE",
      "metadata": {
        "id": "GD4L7pjf0xHE"
      },
      "outputs": [],
      "source": [
        "# Create a generator that yields Pandas DataFrames containing the data from each JSON file\n",
        "cls_dataframes = (pd.read_json(f, orient='index').transpose() for f in tqdm(annotation_file_paths))\n",
        "\n",
        "# Concatenate the DataFrames into a single DataFrame\n",
        "annotation_df = pd.concat(cls_dataframes, ignore_index=False)\n",
        "\n",
        "# Assign the image file name as the index for each row\n",
        "annotation_df['index'] = annotation_df.apply(lambda row: row['imagePath'].split('.')[0], axis=1)\n",
        "annotation_df = annotation_df.set_index('index')\n",
        "\n",
        "# Get the common keys between img_dict and annotation_df\n",
        "common_keys = list(set(img_dict.keys()) & set(annotation_df.index))\n",
        "\n",
        "# Keep only the rows that correspond to the common keys\n",
        "annotation_df = annotation_df.loc[common_keys]\n",
        "\n",
        "shapes_df = annotation_df['shapes'].explode().to_frame().shapes.apply(pd.Series)\n",
        "\n",
        "class_names = shapes_df['label'].unique().tolist()\n",
        "\n",
        "class_names = ['background'] + class_names\n",
        "\n",
        "class_counts = shapes_df['label'].value_counts()\n",
        "\n",
        "font_file = 'KFOlCnqEu92Fr1MmEU9vAw.ttf'\n",
        "\n",
        "# Generate a list of colors with a length equal to the number of labels\n",
        "colors = distinctipy.get_colors(len(class_names))\n",
        "\n",
        "# Make a copy of the color map in integer format\n",
        "int_colors = [tuple(int(c*255) for c in color) for color in colors]\n",
        "\n",
        "draw_bboxes = partial(draw_bounding_boxes, fill=False, font=font_file, width=2, font_size=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce54db2a",
      "metadata": {
        "id": "ce54db2a"
      },
      "source": [
        "### To test the annotations, we take an ID of an image from our list and open it as an RGB file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xQEkBO-605NN",
      "metadata": {
        "id": "xQEkBO-605NN"
      },
      "outputs": [],
      "source": [
        "# Get the file ID of the first image file\n",
        "file_id = list(img_dict.keys())[16]\n",
        "\n",
        "# Open the associated image file as a RGB image\n",
        "sample_img = Image.open(img_dict[file_id]).convert('RGB')\n",
        "\n",
        "# Print the dimensions of the image\n",
        "print(f\"Image Dims: {sample_img.size}\")\n",
        "\n",
        "# Show the image\n",
        "sample_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oZISsFu21UXH",
      "metadata": {
        "id": "oZISsFu21UXH"
      },
      "outputs": [],
      "source": [
        "# Get the row from the 'annotation_df' DataFrame corresponding to the 'file_id'\n",
        "annotation_df.loc[file_id].to_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BeZN6aM11Zse",
      "metadata": {
        "id": "BeZN6aM11Zse"
      },
      "outputs": [],
      "source": [
        "def create_polygon_mask(image_size, vertices):\n",
        "    # Create a new black image with the given dimensions\n",
        "    mask_img = Image.new('L', image_size, 0)\n",
        "\n",
        "    # Draw the polygon on the image. The area inside the polygon will be white (255).\n",
        "    ImageDraw.Draw(mask_img, 'L').polygon(vertices, fill=(255))\n",
        "\n",
        "    # Return the image with the drawn polygon\n",
        "    return mask_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y1vyV6IB1gIO",
      "metadata": {
        "id": "Y1vyV6IB1gIO"
      },
      "outputs": [],
      "source": [
        "# Extract the labels for the sample\n",
        "labels = [shape['label'] for shape in annotation_df.loc[file_id]['shapes']]\n",
        "\n",
        "# Extract the polygon points for segmentation mask\n",
        "shape_points = [shape['points'] for shape in annotation_df.loc[file_id]['shapes']]\n",
        "\n",
        "# Format polygon points for PIL\n",
        "xy_coords = [[tuple(p) for p in points] for points in shape_points]\n",
        "# Generate mask images from polygons\n",
        "mask_imgs = [create_polygon_mask(sample_img.size, xy) for xy in xy_coords]\n",
        "# Convert mask images to tensors\n",
        "masks = torch.concat([Mask(transforms.PILToTensor()(mask_img), dtype=torch.bool) for mask_img in mask_imgs])\n",
        "# Generate bounding box annotations from segmentation masks\n",
        "bboxes = torchvision.ops.masks_to_boxes(masks)\n",
        "\n",
        "# Annotate the sample image with segmentation masks\n",
        "annotated_tensor = draw_segmentation_masks(\n",
        "    image=transforms.PILToTensor()(sample_img),\n",
        "    masks=masks,\n",
        "    alpha=0.3,\n",
        "    colors=[int_colors[i] for i in [class_names.index(label) for label in labels]]\n",
        ")\n",
        "\n",
        "# Annotate the sample image with labels and bounding boxes\n",
        "annotated_tensor = draw_bboxes(\n",
        "    image=annotated_tensor,\n",
        "    boxes=bboxes,\n",
        "    labels=labels,\n",
        "    colors=[int_colors[i] for i in [class_names.index(label) for label in labels]]\n",
        ")\n",
        "\n",
        "tensor_to_pil(annotated_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y3DJtiMU1lHi",
      "metadata": {
        "id": "y3DJtiMU1lHi"
      },
      "source": [
        "### Load MaskRCNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hZiWLLhm1gFu",
      "metadata": {
        "id": "hZiWLLhm1gFu"
      },
      "outputs": [],
      "source": [
        "# Initialize a Mask R-CNN model with pretrained weights\n",
        "model = maskrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
        "\n",
        "# Get the number of input features for the classifier\n",
        "in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "\n",
        "# Get the numbner of output channels for the Mask Predictor\n",
        "dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
        "\n",
        "# Replace the box predictor\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features_box, num_classes=len(class_names))\n",
        "\n",
        "# Replace the mask predictor\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels=in_features_mask, dim_reduced=dim_reduced, num_classes=len(class_names))\n",
        "\n",
        "# Set the model's device and data type, put the data to GPU devices\n",
        "model.to(device=device, dtype=dtype);\n",
        "\n",
        "# Add attributes to store the device and model name for later reference\n",
        "model.device = device\n",
        "model.name = 'maskrcnn_resnet50_fpn_v2'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mw_RoWwi2FyF",
      "metadata": {
        "id": "Mw_RoWwi2FyF"
      },
      "outputs": [],
      "source": [
        "#### Crashes\n",
        "\n",
        "test_inp = torch.randn(1, 3, 256, 256).to(device)\n",
        "summary_df = markdown_to_pandas(f\"{get_module_summary(model.eval(), [test_inp])}\")\n",
        "\n",
        "# # Filter the summary to only contain Conv2d layers and the model\n",
        "summary_df = summary_df[summary_df.index == 0]\n",
        "\n",
        "summary_df.drop(['In size', 'Out size', 'Contains Uninitialized Parameters?'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MFz8Ct1z2Fti",
      "metadata": {
        "id": "MFz8Ct1z2Fti"
      },
      "outputs": [],
      "source": [
        "# Get the list of image IDs\n",
        "img_keys = list(img_dict.keys())\n",
        "\n",
        "# Shuffle the image IDs\n",
        "random.shuffle(img_keys)\n",
        "\n",
        "# Define the percentage of the images that should be used for training\n",
        "train_pct = 0.8\n",
        "val_pct = 0.2\n",
        "\n",
        "# Calculate the index at which to split the subset of image paths into training and validation sets\n",
        "train_split = int(len(img_keys)*train_pct)\n",
        "val_split = int(len(img_keys)*(train_pct+val_pct))\n",
        "\n",
        "# Split the subset of image paths into training and validation sets\n",
        "train_keys = img_keys[:train_split]\n",
        "val_keys = img_keys[train_split:]\n",
        "\n",
        "# Print the number of images in the training and validation sets\n",
        "pd.Series({\n",
        "    \"Training Samples:\": len(train_keys),\n",
        "    \"Validation Samples:\": len(val_keys)\n",
        "}).to_frame().style.hide(axis='columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F7Pw2pte1gAK",
      "metadata": {
        "id": "F7Pw2pte1gAK"
      },
      "outputs": [],
      "source": [
        "# Create a RandomIoUCrop object\n",
        "iou_crop = CustomRandomIoUCrop(min_scale=0.3,\n",
        "                               max_scale=1.0,\n",
        "                               min_aspect_ratio=0.5,\n",
        "                               max_aspect_ratio=2.0,\n",
        "                               sampler_options=[0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
        "                               trials=400,\n",
        "                               jitter_factor=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbE94FH32Sjb",
      "metadata": {
        "id": "dbE94FH32Sjb"
      },
      "outputs": [],
      "source": [
        "# Set training image size\n",
        "train_sz = 512\n",
        "\n",
        "# Create a `ResizeMax` object\n",
        "resize_max = ResizeMax(max_sz=train_sz)\n",
        "\n",
        "# Create a `PadSquare` object\n",
        "pad_square = PadSquare(shift=True, fill=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ne9LW4Bl2SdZ",
      "metadata": {
        "id": "ne9LW4Bl2SdZ"
      },
      "outputs": [],
      "source": [
        "# Extract the labels for the sample\n",
        "labels = [shape['label'] for shape in annotation_df.loc[file_id]['shapes']]\n",
        "# Extract the polygon points for segmentation mask\n",
        "shape_points = [shape['points'] for shape in annotation_df.loc[file_id]['shapes']]\n",
        "# Format polygon points for PIL\n",
        "xy_coords = [[tuple(p) for p in points] for points in shape_points]\n",
        "# Generate mask images from polygons\n",
        "mask_imgs = [create_polygon_mask(sample_img.size, xy) for xy in xy_coords]\n",
        "# Convert mask images to tensors\n",
        "masks = torch.concat([Mask(transforms.PILToTensor()(mask_img), dtype=torch.bool) for mask_img in mask_imgs])\n",
        "# Generate bounding box annotations from segmentation masks\n",
        "bboxes = BoundingBoxes(data=torchvision.ops.masks_to_boxes(masks), format='xyxy', canvas_size=sample_img.size[::-1])\n",
        "\n",
        "# Get colors for dataset sample\n",
        "sample_colors = [int_colors[i] for i in [class_names.index(label) for label in labels]]\n",
        "\n",
        "# Prepare mask and bounding box targets\n",
        "targets = {\n",
        "    'masks': Mask(masks),\n",
        "    'boxes': bboxes,\n",
        "    'labels': torch.Tensor([class_names.index(label) for label in labels])\n",
        "}\n",
        "\n",
        "# Crop the image\n",
        "cropped_img, targets = iou_crop(sample_img, targets)\n",
        "\n",
        "# Resize the image\n",
        "resized_img, targets = resize_max(cropped_img, targets)\n",
        "\n",
        "# Pad the image\n",
        "padded_img, targets = pad_square(resized_img, targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ieBUgXgG2SaN",
      "metadata": {
        "id": "ieBUgXgG2SaN"
      },
      "outputs": [],
      "source": [
        "# Ensure the padded image is the target size\n",
        "resize = Resize([train_sz] * 2, antialias=True)\n",
        "\n",
        "resized_padded_img, targets = resize(padded_img, targets)\n",
        "sanitized_img, targets = SanitizeBoundingBoxes()(resized_padded_img, targets)\n",
        "\n",
        "# Annotate the sample image with segmentation masks\n",
        "annotated_tensor = draw_segmentation_masks(\n",
        "    image=transforms.PILToTensor()(sanitized_img),\n",
        "    masks=targets['masks'],\n",
        "    alpha=0.3,\n",
        "    colors=sample_colors\n",
        ")\n",
        "\n",
        "# Annotate the sample image with labels and bounding boxes\n",
        "annotated_tensor = draw_bboxes(\n",
        "    image=annotated_tensor,\n",
        "    boxes=targets['boxes'],\n",
        "    labels=[class_names[int(label.item())] for label in targets['labels']],\n",
        "    colors=sample_colors\n",
        ")\n",
        "\n",
        "# # Display the annotated image\n",
        "display(tensor_to_pil(annotated_tensor))\n",
        "\n",
        "pd.Series({\n",
        "    \"Source Image:\": sample_img.size,\n",
        "    \"Cropped Image:\": cropped_img.size,\n",
        "    \"Resized Image:\": resized_img.size,\n",
        "    \"Padded Image:\": padded_img.size,\n",
        "    \"Resized Padded Image:\": resized_padded_img.size,\n",
        "}).to_frame().style.hide(axis='columns')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7h-DSBE2b0P",
      "metadata": {
        "id": "a7h-DSBE2b0P"
      },
      "source": [
        "#### Training Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EBC4I4nI2Zr7",
      "metadata": {
        "id": "EBC4I4nI2Zr7"
      },
      "outputs": [],
      "source": [
        "class TreeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This class represents a PyTorch Dataset for a collection of images and their annotations.\n",
        "    The class is designed to load images along with their corresponding segmentation masks, bounding box annotations, and labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_keys, annotation_df, img_dict, class_to_idx, transforms=None):\n",
        "        \"\"\"\n",
        "        Constructor for the TreeDataset class.\n",
        "\n",
        "        Parameters:\n",
        "        img_keys (list): List of unique identifiers for images.\n",
        "        annotation_df (DataFrame): DataFrame containing the image annotations.\n",
        "        img_dict (dict): Dictionary mapping image identifiers to image file paths.\n",
        "        class_to_idx (dict): Dictionary mapping class labels to indices.\n",
        "        transforms (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        super(Dataset, self).__init__()\n",
        "\n",
        "        self._img_keys = img_keys  # List of image keys\n",
        "        self._annotation_df = annotation_df  # DataFrame containing annotations\n",
        "        self._img_dict = img_dict  # Dictionary mapping image keys to image paths\n",
        "        self._class_to_idx = class_to_idx  # Dictionary mapping class names to class indices\n",
        "        self._transforms = transforms  # Image transforms to be applied\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset.\n",
        "\n",
        "        Returns:\n",
        "        int: The number of items in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self._img_keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Fetch an item from the dataset at the specified index.\n",
        "\n",
        "        Parameters:\n",
        "        index (int): Index of the item to fetch from the dataset.\n",
        "\n",
        "        Returns:\n",
        "        tuple: A tuple containing the image and its associated target (annotations).\n",
        "        \"\"\"\n",
        "        # Retrieve the key for the image at the specified index\n",
        "        img_key = self._img_keys[index]\n",
        "\n",
        "        # Get the annotations for this image\n",
        "        annotation = self._annotation_df.loc[img_key]\n",
        "\n",
        "        # Load the image and its target (segmentation masks, bounding boxes and labels)\n",
        "        image, target = self._load_image_and_target(annotation)\n",
        "\n",
        "        # Apply the transformations, if any\n",
        "        if self._transforms:\n",
        "            image, target = self._transforms(image, target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def _load_image_and_target(self, annotation):\n",
        "        \"\"\"\n",
        "        Load an image and its target (bounding boxes and labels).\n",
        "\n",
        "        Parameters:\n",
        "        annotation (pandas.Series): The annotations for an image.\n",
        "\n",
        "        Returns:\n",
        "        tuple: A tuple containing the image and a dictionary with 'boxes' and 'labels' keys.\n",
        "        \"\"\"\n",
        "        # Retrieve the file path of the image\n",
        "        filepath = self._img_dict[annotation.name]\n",
        "\n",
        "        # Open the image file and convert it to RGB\n",
        "        image = Image.open(filepath).convert('RGB')\n",
        "\n",
        "        # Convert the class labels to indices\n",
        "        labels = [shape['label'] for shape in annotation['shapes']]\n",
        "        labels = torch.Tensor([self._class_to_idx[label] for label in labels])\n",
        "        labels = labels.to(dtype=torch.int64)\n",
        "\n",
        "        # Convert polygons to mask images\n",
        "        shape_points = [shape['points'] for shape in annotation['shapes']]\n",
        "        xy_coords = [[tuple(p) for p in points] for points in shape_points]\n",
        "        mask_imgs = [create_polygon_mask(image.size, xy) for xy in xy_coords]\n",
        "        masks = Mask(torch.concat([Mask(transforms.PILToTensor()(mask_img), dtype=torch.bool) for mask_img in mask_imgs]))\n",
        "\n",
        "        # Generate bounding box annotations from segmentation masks\n",
        "        bboxes = BoundingBoxes(data=torchvision.ops.masks_to_boxes(masks), format='xyxy', canvas_size=image.size[::-1])\n",
        "\n",
        "        return image, {'masks': masks,'boxes': bboxes, 'labels': labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MG6BnIWo2Zmu",
      "metadata": {
        "id": "MG6BnIWo2Zmu"
      },
      "outputs": [],
      "source": [
        "# Compose transforms for data augmentation\n",
        "data_aug_tfms = transforms.Compose(\n",
        "    transforms=[\n",
        "        iou_crop,\n",
        "        transforms.ColorJitter(\n",
        "                brightness = (0.875, 1.125),\n",
        "                contrast = (0.5, 1.5),\n",
        "                saturation = (0.5, 1.5),\n",
        "                hue = (-0.05, 0.05),\n",
        "        ),\n",
        "        transforms.RandomGrayscale(),\n",
        "        transforms.RandomEqualize(),\n",
        "        transforms.RandomPosterize(bits=3, p=0.5),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Compose transforms to resize and pad input images\n",
        "resize_pad_tfm = transforms.Compose([\n",
        "    resize_max,\n",
        "    pad_square,\n",
        "    transforms.Resize([train_sz] * 2, antialias=True)\n",
        "])\n",
        "\n",
        "# Compose transforms to sanitize bounding boxes and normalize input data\n",
        "final_tfms = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.SanitizeBoundingBoxes(),\n",
        "])\n",
        "\n",
        "# # Define the transformations for training and validation datasets\n",
        "train_tfms = transforms.Compose([\n",
        "    data_aug_tfms,\n",
        "    resize_pad_tfm,\n",
        "    final_tfms\n",
        "])\n",
        "\n",
        "valid_tfms = transforms.Compose([resize_pad_tfm, final_tfms])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fJUl4Jo02fsZ",
      "metadata": {
        "id": "fJUl4Jo02fsZ"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from class names to class indices\n",
        "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
        "\n",
        "# Instantiate the datasets using the defined transformations\n",
        "train_dataset = TreeDataset(train_keys, annotation_df, img_dict, class_to_idx, train_tfms)\n",
        "valid_dataset = TreeDataset(val_keys, annotation_df, img_dict, class_to_idx, valid_tfms)\n",
        "\n",
        "# Print the number of samples in the training and validation datasets\n",
        "pd.Series({\n",
        "    'Training dataset size:': len(train_dataset),\n",
        "    'Validation dataset size:': len(valid_dataset)}\n",
        ").to_frame().style.hide(axis='columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qPmJpe2V2fle",
      "metadata": {
        "id": "qPmJpe2V2fle"
      },
      "outputs": [],
      "source": [
        "dataset_sample = train_dataset[0]\n",
        "\n",
        "# Get colors for dataset sample\n",
        "sample_colors = [int_colors[int(i.item())] for i in dataset_sample[1]['labels']]\n",
        "\n",
        "# Annotate the sample image with segmentation masks\n",
        "annotated_tensor = draw_segmentation_masks(\n",
        "    image=(dataset_sample[0]*255).to(dtype=torch.uint8),\n",
        "    masks=dataset_sample[1]['masks'],\n",
        "    alpha=0.3,\n",
        "    colors=sample_colors\n",
        ")\n",
        "\n",
        "# Annotate the sample image with bounding boxes\n",
        "annotated_tensor = draw_bboxes(\n",
        "    image=annotated_tensor,\n",
        "    boxes=dataset_sample[1]['boxes'],\n",
        "    labels=[class_names[int(i.item())] for i in dataset_sample[1]['labels']],\n",
        "    colors=sample_colors\n",
        ")\n",
        "\n",
        "tensor_to_pil(annotated_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ggknAO-42lXk",
      "metadata": {
        "id": "ggknAO-42lXk"
      },
      "outputs": [],
      "source": [
        "# Set the training batch size\n",
        "bs = 4\n",
        "\n",
        "# Set the number of worker processes for loading data.\n",
        "num_workers = multiprocessing.cpu_count()//2\n",
        "\n",
        "# Define parameters for DataLoader\n",
        "data_loader_params = {\n",
        "    'batch_size': bs,  # Batch size for data loading\n",
        "    'num_workers': num_workers,  # Number of subprocesses to use for data loading\n",
        "    'persistent_workers': True,  # If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the worker dataset instances alive.\n",
        "    # 'pin_memory': 'cuda' in device,  # If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.\n",
        "    # 'pin_memory_device': device if 'cuda' in device else '',  # Specifies the device where the data should be loaded. Commonly set to use the GPU.\n",
        "    'pin_memory': 'cuda',  # If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.\n",
        "    'pin_memory_device': 'cuda',  # Specifies the device where the data should be loaded. Commonly set to use the GPU.\n",
        "\n",
        "    'collate_fn': lambda batch: tuple(zip(*batch)),\n",
        "}\n",
        "# Create DataLoader for training data. Data is shuffled for every epoch.\n",
        "train_dataloader = DataLoader(train_dataset, **data_loader_params, shuffle=True)\n",
        "\n",
        "# Create DataLoader for validation data. Shuffling is not necessary for validation data.\n",
        "valid_dataloader = DataLoader(valid_dataset, **data_loader_params)\n",
        "\n",
        "# Print the number of batches in the training and validation DataLoaders\n",
        "pd.Series({\n",
        "    'Number of batches in train DataLoader:': len(train_dataloader),\n",
        "    'Number of batches in validation DataLoader:': len(valid_dataloader)}\n",
        ").to_frame().style.hide(axis='columns')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RT34yLRl2n0d",
      "metadata": {
        "id": "RT34yLRl2n0d"
      },
      "source": [
        "#### Fine-tuning the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DZ-X9-nv2lWR",
      "metadata": {
        "id": "DZ-X9-nv2lWR"
      },
      "outputs": [],
      "source": [
        "def run_epoch(model, dataloader, optimizer, lr_scheduler, device, scaler, epoch_id, is_training):\n",
        "    \"\"\"\n",
        "    Function to run a single training or evaluation epoch.\n",
        "\n",
        "    Args:\n",
        "        model: A PyTorch model to train or evaluate.\n",
        "        dataloader: A PyTorch DataLoader providing the data.\n",
        "        optimizer: The optimizer to use for training the model.\n",
        "        loss_func: The loss function used for training.\n",
        "        device: The device (CPU or GPU) to run the model on.\n",
        "        scaler: Gradient scaler for mixed-precision training.\n",
        "        is_training: Boolean flag indicating whether the model is in training or evaluation mode.\n",
        "\n",
        "    Returns:\n",
        "        The average loss for the epoch.\n",
        "    \"\"\"\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0  # Initialize the total loss for this epoch\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Eval\")  # Initialize a progress bar\n",
        "\n",
        "    # Loop over the data\n",
        "    for batch_id, (inputs, targets) in enumerate(dataloader):\n",
        "        # Move inputs and targets to the specified device\n",
        "        inputs = torch.stack(inputs).to(device)\n",
        "\n",
        "        # Forward pass with Automatic Mixed Precision (AMP) context manager\n",
        "        with autocast(torch.device(device).type):\n",
        "            if is_training:\n",
        "                losses = model(inputs.to(device), move_data_to_device(targets, device))\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    losses = model(inputs.to(device), move_data_to_device(targets, device))\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = sum([loss for loss in losses.values()])  # Sum up the losses\n",
        "\n",
        "        # If in training mode, backpropagate the error and update the weights\n",
        "        if is_training:\n",
        "            if scaler:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                old_scaler = scaler.get_scale()\n",
        "                scaler.update()\n",
        "                new_scaler = scaler.get_scale()\n",
        "                if new_scaler >= old_scaler:\n",
        "                    lr_scheduler.step()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Update the total loss\n",
        "        loss_item = loss.item()\n",
        "        epoch_loss += loss_item\n",
        "\n",
        "        # Update the progress bar\n",
        "        progress_bar_dict = dict(loss=loss_item, avg_loss=epoch_loss/(batch_id+1))\n",
        "        if is_training:\n",
        "            progress_bar_dict.update(lr=lr_scheduler.get_last_lr()[0])\n",
        "        progress_bar.set_postfix(progress_bar_dict)\n",
        "        progress_bar.update()\n",
        "\n",
        "        # If loss is NaN or infinity, stop training\n",
        "        if is_training:\n",
        "            stop_training_message = f\"Loss is NaN or infinite at epoch {epoch_id}, batch {batch_id}. Stopping training.\"\n",
        "            assert not math.isnan(loss_item) and math.isfinite(loss_item), stop_training_message\n",
        "\n",
        "    # Cleanup and close the progress bar\n",
        "    progress_bar.close()\n",
        "\n",
        "    # Return the average loss for this epoch\n",
        "    return epoch_loss / (batch_id + 1)\n",
        "\n",
        "def train_loop(model,\n",
        "               train_dataloader,\n",
        "               valid_dataloader,\n",
        "               optimizer,\n",
        "               lr_scheduler,\n",
        "               device,\n",
        "               epochs,\n",
        "               checkpoint_path,\n",
        "               use_scaler=False):\n",
        "    \"\"\"\n",
        "    Main training loop.\n",
        "\n",
        "    Args:\n",
        "        model: A PyTorch model to train.\n",
        "        train_dataloader: A PyTorch DataLoader providing the training data.\n",
        "        valid_dataloader: A PyTorch DataLoader providing the validation data.\n",
        "        optimizer: The optimizer to use for training the model.\n",
        "        lr_scheduler: The learning rate scheduler.\n",
        "        device: The device (CPU or GPU) to run the model on.\n",
        "        epochs: The number of epochs to train for.\n",
        "        checkpoint_path: The path where to save the best model checkpoint.\n",
        "        use_scaler: Whether to scale graidents when using a CUDA device\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' and use_scaler else None\n",
        "    best_loss = float('inf')  # Initialize the best validation loss\n",
        "\n",
        "    # Loop over the epochs\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "        # Run a training epoch and get the training loss\n",
        "        train_loss = run_epoch(model, train_dataloader, optimizer, lr_scheduler, device, scaler, epoch, is_training=True)\n",
        "\n",
        "        # Run an evaluation epoch and get the validation loss\n",
        "        with torch.no_grad():\n",
        "            valid_loss = run_epoch(model, valid_dataloader, None, None, device, scaler, epoch, is_training=False)\n",
        "\n",
        "        # If the validation loss is lower than the best validation loss seen so far, save the model checkpoint\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "            # Save metadata about the training process\n",
        "            training_metadata = {\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'valid_loss': valid_loss,\n",
        "                'learning_rate': lr_scheduler.get_last_lr()[0],\n",
        "                'model_architecture': model.name\n",
        "            }\n",
        "            with open(Path(checkpoint_path.parent/'training_metadata.json'), 'w') as f:\n",
        "                json.dump(training_metadata, f)\n",
        "\n",
        "    # If the device is a GPU, empty the cache\n",
        "    if device.type != 'cpu':\n",
        "        getattr(torch, device.type).empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7vUYNgmL2lPp",
      "metadata": {
        "id": "7vUYNgmL2lPp"
      },
      "outputs": [],
      "source": [
        "project_name = 'maskrcnn'\n",
        "# The path for the project folder\n",
        "project_dir = Path(f\"./{project_name}/\")\n",
        "\n",
        "# Create the project directory if it does not already exist\n",
        "project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.name = \"MaskRCNN_custom\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jMUfBHQu2lNC",
      "metadata": {
        "id": "jMUfBHQu2lNC"
      },
      "outputs": [],
      "source": [
        "# Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# Create a directory to store the checkpoints if it does not already exist\n",
        "checkpoint_dir = Path(project_dir/f\"{timestamp}\")\n",
        "\n",
        "# Create the checkpoint directory if it does not already exist\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# The model checkpoint path\n",
        "checkpoint_path = checkpoint_dir/f\"{model.name}.pth\"\n",
        "\n",
        "print(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-jY5YT7W2uMq",
      "metadata": {
        "id": "-jY5YT7W2uMq"
      },
      "outputs": [],
      "source": [
        "# Create a color map and write it to a JSON file\n",
        "color_map = {'items': [{'label': label, 'color': color} for label, color in zip(class_names, colors)]}\n",
        "with open(f\"{checkpoint_dir}/{dataset_path.name}-colormap.json\", \"w\") as file:\n",
        "    json.dump(color_map, file)\n",
        "\n",
        "# Print the name of the file that the color map was written to\n",
        "print(f\"{checkpoint_dir}/{dataset_path.name}-colormap.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Oo39kHu2uHK",
      "metadata": {
        "id": "_Oo39kHu2uHK"
      },
      "outputs": [],
      "source": [
        "# Learning rate for the model\n",
        "lr = 5e-4\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 40\n",
        "\n",
        "# AdamW optimizer; includes weight decay for regularization\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# Learning rate scheduler; adjusts the learning rate during training\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
        "                                                   max_lr=lr,\n",
        "                                                   total_steps=epochs*len(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dsn9d6mX2x_g",
      "metadata": {
        "id": "dsn9d6mX2x_g"
      },
      "outputs": [],
      "source": [
        "train_loop(model=model,\n",
        "           train_dataloader=train_dataloader,\n",
        "           valid_dataloader=valid_dataloader,\n",
        "           optimizer=optimizer,\n",
        "           lr_scheduler=lr_scheduler,\n",
        "           device=torch.device(device),\n",
        "           epochs=epochs,\n",
        "           checkpoint_path=checkpoint_path,\n",
        "           use_scaler=True)\n",
        "\n",
        "torch.save(model.state_dict(), Path(\"/content/drive/MyDrive/AI Final\") / 'final_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RjjYBous7stA",
      "metadata": {
        "id": "RjjYBous7stA"
      },
      "source": [
        "### Testing and Validating a pre-labelled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uAENPzhQ2x6V",
      "metadata": {
        "id": "uAENPzhQ2x6V"
      },
      "outputs": [],
      "source": [
        "# Choose a random item from the validation set\n",
        "file_id = random.choice(val_keys)\n",
        "\n",
        "# Retrieve the image file path associated with the file ID\n",
        "test_file = img_dict[file_id]\n",
        "\n",
        "# Open the test file\n",
        "test_img = Image.open(test_file).convert('RGB')\n",
        "\n",
        "#test_img = Image.open('/content/drive/MyDrive/AI Final/youtube_frames/almost crashed so we hitchhiked - 13/almost crashed so we hitchhiked_time0000s.jpg').convert('RGB')\n",
        "\n",
        "# Resize the test image\n",
        "input_img = resize_img(test_img, target_sz=train_sz, divisor=1)\n",
        "\n",
        "# Calculate the scale between the source image and the resized image\n",
        "min_img_scale = min(test_img.size) / min(input_img.size)\n",
        "\n",
        "display(test_img)\n",
        "\n",
        "# Print the prediction data as a Pandas DataFrame for easy formatting\n",
        "pd.Series({\n",
        "    \"Source Image Size:\": test_img.size,\n",
        "    \"Input Dims:\": input_img.size,\n",
        "    \"Min Image Scale:\": min_img_scale,\n",
        "    \"Input Image Size:\": input_img.size\n",
        "}).to_frame().style.hide(axis='columns')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FSMBbyJt25UI",
      "metadata": {
        "id": "FSMBbyJt25UI"
      },
      "outputs": [],
      "source": [
        "# Extract the polygon points for segmentation mask\n",
        "target_shape_points = [shape['points'] for shape in annotation_df.loc[file_id]['shapes']]\n",
        "# Format polygon points for PIL\n",
        "target_xy_coords = [[tuple(p) for p in points] for points in target_shape_points]\n",
        "# Generate mask images from polygons\n",
        "target_mask_imgs = [create_polygon_mask(test_img.size, xy) for xy in target_xy_coords]\n",
        "# Convert mask images to tensors\n",
        "target_masks = Mask(torch.concat([Mask(transforms.PILToTensor()(mask_img), dtype=torch.bool) for mask_img in target_mask_imgs]))\n",
        "\n",
        "# Get the target labels and bounding boxes\n",
        "target_labels = [shape['label'] for shape in annotation_df.loc[file_id]['shapes']]\n",
        "target_bboxes = BoundingBoxes(data=torchvision.ops.masks_to_boxes(target_masks), format='xyxy', canvas_size=test_img.size[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNIhomul25RA",
      "metadata": {
        "id": "MNIhomul25RA"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Ensure the model and input data are on the same device\n",
        "model.to(device)\n",
        "input_tensor = transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)])(input_img)[None].to(device)\n",
        "\n",
        "# Make a prediction with the model\n",
        "with torch.no_grad():\n",
        "    model_output = model(input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gf_eQIOf1f9j",
      "metadata": {
        "id": "gf_eQIOf1f9j"
      },
      "outputs": [],
      "source": [
        "# Set the confidence threshold\n",
        "threshold = 0.8\n",
        "\n",
        "# Move model output to the CPU\n",
        "model_output = move_data_to_device(model_output, 'cpu')\n",
        "\n",
        "# Filter the output based on the confidence threshold\n",
        "scores_mask = model_output[0]['scores'] > threshold\n",
        "\n",
        "# Scale the predicted bounding boxes\n",
        "pred_bboxes = BoundingBoxes(model_output[0]['boxes'][scores_mask]*min_img_scale, format='xyxy', canvas_size=input_img.size[::-1])\n",
        "\n",
        "# Get the class names for the predicted label indices\n",
        "pred_labels = [class_names[int(label)] for label in model_output[0]['labels'][scores_mask]]\n",
        "\n",
        "# Extract the confidence scores\n",
        "pred_scores = model_output[0]['scores']\n",
        "\n",
        "# Scale and stack the predicted segmentation masks\n",
        "pred_masks = F.interpolate(model_output[0]['masks'][scores_mask], size=test_img.size[::-1])\n",
        "pred_masks = torch.concat([Mask(torch.where(mask >= threshold, 1, 0), dtype=torch.bool) for mask in pred_masks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8mwMeArF29hf",
      "metadata": {
        "id": "8mwMeArF29hf"
      },
      "outputs": [],
      "source": [
        "# Get the annotation colors for the targets and predictions\n",
        "target_colors=[int_colors[i] for i in [class_names.index(label) for label in target_labels]]\n",
        "pred_colors=[int_colors[i] for i in [class_names.index(label) for label in pred_labels]]\n",
        "\n",
        "# Convert the test images to a tensor\n",
        "img_tensor = transforms.PILToTensor()(test_img)\n",
        "\n",
        "# Annotate the test image with the target segmentation masks\n",
        "annotated_tensor = draw_segmentation_masks(image=img_tensor, masks=target_masks, alpha=0.3, colors=target_colors)\n",
        "# Annotate the test image with the target bounding boxes\n",
        "annotated_tensor = draw_bboxes(image=annotated_tensor, boxes=target_bboxes, labels=target_labels, colors=target_colors)\n",
        "# Display the annotated test image\n",
        "annotated_test_img = tensor_to_pil(annotated_tensor)\n",
        "\n",
        "# Annotate the test image with the predicted segmentation masks\n",
        "annotated_tensor = draw_segmentation_masks(image=img_tensor, masks=pred_masks, alpha=0.3, colors=pred_colors)\n",
        "# Annotate the test image with the predicted labels and bounding boxes\n",
        "annotated_tensor = draw_bboxes(\n",
        "    image=annotated_tensor,\n",
        "    boxes=pred_bboxes,\n",
        "    labels=[f\"{label}\\n{prob*100:.2f}%\" for label, prob in zip(pred_labels, pred_scores)],\n",
        "    colors=pred_colors\n",
        ")\n",
        "\n",
        "# Display the annotated test image with the predicted bounding boxes\n",
        "display(stack_imgs([annotated_test_img, tensor_to_pil(annotated_tensor)]))\n",
        "\n",
        "# Print the prediction data as a Pandas DataFrame for easy formatting\n",
        "pd.Series({\n",
        "    \"Target BBoxes:\": [f\"{label}:{bbox}\" for label, bbox in zip(target_labels, np.round(target_bboxes.numpy(), decimals=3))],\n",
        "    \"Predicted BBoxes:\": [f\"{label}:{bbox}\" for label, bbox in zip(pred_labels, pred_bboxes.round(decimals=3).numpy())],\n",
        "    \"Confidence Scores:\": [f\"{label}: {prob*100:.2f}%\" for label, prob in zip(pred_labels, pred_scores)]\n",
        "}).to_frame().style.hide(axis='columns')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R-z1Lnj0b73J",
      "metadata": {
        "id": "R-z1Lnj0b73J"
      },
      "source": [
        "### Calculating Occurrence of Label with Uncertainty of Accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tHkeP794RyYY",
      "metadata": {
        "id": "tHkeP794RyYY"
      },
      "outputs": [],
      "source": [
        "allPred = zip(pred_labels, pred_bboxes.round(decimals=3).numpy().tolist())\n",
        "allConf = zip(pred_labels, pred_scores)\n",
        "\n",
        "# Create a DataFrame to store occurrences and uncertainty\n",
        "occur_df = pd.DataFrame(columns=['label', 'count', 'uncertainty'])\n",
        "\n",
        "# Process predictions and confidence scores together\n",
        "for (label, bbox), (conf_label, conf_score) in zip(allPred, allConf):\n",
        "    # Ensure labels match (sanity check)\n",
        "    assert label == conf_label, \"Labels in predictions and confidence scores don't match\"\n",
        "\n",
        "    # Find or create row for this label\n",
        "    if label not in occur_df['label'].values:\n",
        "        occur_df = pd.concat([occur_df, pd.DataFrame({\n",
        "            'label': [label],\n",
        "            'count': [0],\n",
        "            'uncertainty': [0]\n",
        "        })], ignore_index=True)\n",
        "\n",
        "    # Update count\n",
        "    occur_df.loc[occur_df['label'] == label, 'count'] += 1\n",
        "\n",
        "    # Update uncertainty if confidence is below threshold\n",
        "    if conf_score < 0.97:\n",
        "        occur_df.loc[occur_df['label'] == label, 'uncertainty'] -= 1\n",
        "\n",
        "    print(f\"{bbox}\")\n",
        "\n",
        "# Set label as index for cleaner display\n",
        "occur_df.set_index('label', inplace=True)\n",
        "print(occur_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AG2kujwubh5k",
      "metadata": {
        "id": "AG2kujwubh5k"
      },
      "source": [
        "### Calculating Occurrence of Label (No Uncertainty) eliminating overlap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mGQ8mXNAbtCc",
      "metadata": {
        "id": "mGQ8mXNAbtCc"
      },
      "source": [
        "The code below is an experimental section that considers overlap of bounding boxes in its occurrence caclculation. This code does not take into account the chances of a label being highly uncertain, but attempts (!!!!) to eliminate cases of double counting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8tPlJFDFZaB9",
      "metadata": {
        "id": "8tPlJFDFZaB9"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def compute_overlap(bbox1, bbox2):\n",
        "    \"\"\"Calculate overlap considering both 20% of larger box and full containment\"\"\"\n",
        "    # Check if one box is entirely inside the other\n",
        "    def is_inside(a, b):\n",
        "        return (a[0] >= b[0] and a[2] <= b[2] and\n",
        "                a[1] >= b[1] and a[3] <= b[3])\n",
        "\n",
        "    if is_inside(bbox1, bbox2) or is_inside(bbox2, bbox1):\n",
        "        return 1.0  # Always merge fully contained boxes\n",
        "\n",
        "    # Calculate intersection area\n",
        "    x1 = max(bbox1[0], bbox2[0])\n",
        "    y1 = max(bbox1[1], bbox2[1])\n",
        "    x2 = min(bbox1[2], bbox2[2])\n",
        "    y2 = min(bbox1[3], bbox2[3])\n",
        "\n",
        "    if x2 < x1 or y2 < y1:\n",
        "        return 0.0\n",
        "\n",
        "    intersection_area = (x2 - x1) * (y2 - y1)\n",
        "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
        "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
        "\n",
        "    # Use area of LARGER box as denominator\n",
        "    larger_area = max(area1, area2)\n",
        "    return intersection_area / larger_area if larger_area != 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w3ZALEY0Zcs-",
      "metadata": {
        "id": "w3ZALEY0Zcs-"
      },
      "outputs": [],
      "source": [
        "class UnionFind:\n",
        "    def __init__(self, size):\n",
        "        self.parent = list(range(size))\n",
        "\n",
        "    def find(self, x):\n",
        "        if self.parent[x] != x:\n",
        "            self.parent[x] = self.find(self.parent[x])\n",
        "        return self.parent[x]\n",
        "\n",
        "    def union(self, x, y):\n",
        "        fx = self.find(x)\n",
        "        fy = self.find(y)\n",
        "        if fx != fy:\n",
        "            self.parent[fy] = fx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_61c4NA6ZeX3",
      "metadata": {
        "id": "_61c4NA6ZeX3"
      },
      "outputs": [],
      "source": [
        "# Process predictions\n",
        "allPred = zip(pred_labels, pred_bboxes.round(decimals=3).numpy().tolist())\n",
        "label_to_boxes = defaultdict(list)\n",
        "for label, bbox in allPred:\n",
        "    label_to_boxes[label].append(bbox)\n",
        "\n",
        "occur = {}\n",
        "\n",
        "for label, boxes in label_to_boxes.items():\n",
        "    n = len(boxes)\n",
        "    if n == 0:\n",
        "        occur[label] = 0\n",
        "        continue\n",
        "\n",
        "    uf = UnionFind(n)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            if compute_overlap(boxes[i], boxes[j]) >= 0.2:\n",
        "                uf.union(i, j)\n",
        "\n",
        "    occur[label] = len(set(uf.find(k) for k in range(n)))\n",
        "\n",
        "print(occur)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hv5Uo6eiSkC9",
      "metadata": {
        "id": "hv5Uo6eiSkC9"
      },
      "source": [
        "### Running Image Identification for a All Videos and Saving Occurrences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LWELA8c2rAYs",
      "metadata": {
        "id": "LWELA8c2rAYs"
      },
      "outputs": [],
      "source": [
        "def evaluate_images(img_pth, all_results, filename):\n",
        "    try:\n",
        "        # Open the test file\n",
        "        target_img = Image.open(img_pth).convert('RGB')\n",
        "\n",
        "        # Resize the test image\n",
        "        input_img = resize_img(target_img, target_sz=train_sz, divisor=1)\n",
        "\n",
        "        # Calculate the scale between the source image and the resized image\n",
        "        min_img_scale = min(target_img.size) / min(input_img.size)\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "        input_tensor = transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)])(input_img)[None].to(device)\n",
        "\n",
        "        # Make a prediction with the model\n",
        "        with torch.no_grad():\n",
        "            model_output = model(input_tensor)\n",
        "\n",
        "        # Set the confidence threshold\n",
        "        threshold = 0.8\n",
        "        model_output = move_data_to_device(model_output, 'cpu')\n",
        "        scores_mask = model_output[0]['scores'] > threshold\n",
        "\n",
        "        # Initialize empty counts dictionary\n",
        "        image_counts = {}\n",
        "\n",
        "        if sum(scores_mask) > 0:  # If we have any detections\n",
        "            # Get the detected class names\n",
        "            pred_labels = [class_names[int(label)] for label in model_output[0]['labels'][scores_mask]]\n",
        "\n",
        "            # Initialize counts for detected classes\n",
        "            image_counts = {class_name: 0 for class_name in set(pred_labels)}\n",
        "\n",
        "            # Count occurrences of each class in this image\n",
        "            for label in pred_labels:\n",
        "                image_counts[label] += 1\n",
        "\n",
        "            # Handle masks only if they exist\n",
        "            try:\n",
        "                pred_masks = F.interpolate(model_output[0]['masks'][scores_mask], size=target_img.size[::-1])\n",
        "                pred_masks = torch.concat([Mask(torch.where(mask >= threshold, 1, 0), dtype=torch.bool) for mask in pred_masks])\n",
        "            except RuntimeError:\n",
        "                pass  # Skip mask processing if it fails\n",
        "        else:\n",
        "            # No detections - create empty record\n",
        "            image_counts = {}\n",
        "\n",
        "        # Add record for this image\n",
        "        record = {'filename': filename}\n",
        "        record.update(image_counts)\n",
        "        all_results.append(record)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {str(e)}\")\n",
        "        # Add empty record if processing fails\n",
        "        all_results.append({'filename': filename})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ftsJ9ztKrG9d",
      "metadata": {
        "id": "ftsJ9ztKrG9d"
      },
      "outputs": [],
      "source": [
        "def process_folder(folder_path):\n",
        "    all_results = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            evaluate_images(file_path, all_results, filename)\n",
        "\n",
        "    # Create DataFrame from all results\n",
        "    per_image_df = pd.DataFrame(all_results).fillna(0)\n",
        "\n",
        "    # Get all unique class names from all detected classes\n",
        "    all_classes = [col for col in per_image_df.columns if col != 'filename']\n",
        "\n",
        "    # Ensure all class columns exist (in case some images had zero detections)\n",
        "    for class_name in class_names:\n",
        "        if class_name not in per_image_df.columns:\n",
        "            per_image_df[class_name] = 0\n",
        "\n",
        "    # Reorder columns to have filename first\n",
        "    cols = ['filename'] + class_names\n",
        "    per_image_df = per_image_df[cols]\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary_df = per_image_df.drop('filename', axis=1).agg(['sum', 'mean'])\n",
        "    summary_df = summary_df.transpose().rename(columns={\n",
        "        'sum': 'total_count',\n",
        "        'mean': 'average_per_image'\n",
        "    })\n",
        "\n",
        "    # Save files\n",
        "    per_image_path = os.path.join(folder_path, 'class_counts_per_image.csv')\n",
        "    summary_path = os.path.join(folder_path, 'class_statistics_summary.csv')\n",
        "\n",
        "    per_image_df.to_csv(per_image_path, index=False)\n",
        "    summary_df.to_csv(summary_path)\n",
        "\n",
        "    return per_image_df, summary_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ZKQ4Bz-PiVG",
      "metadata": {
        "id": "2ZKQ4Bz-PiVG"
      },
      "outputs": [],
      "source": [
        "parent_directory = '/content/drive/MyDrive/AI Final/youtube_frames'\n",
        "all_results = {}\n",
        "\n",
        "with os.scandir(parent_directory) as entries:\n",
        "    for entry in entries:\n",
        "        if entry.is_dir():\n",
        "            print(f\"Processing folder: {entry.name}\")\n",
        "            per_image_results, total_results = process_folder(entry.path)\n",
        "            all_results[entry.name] = {\n",
        "                'per_image_results': per_image_results,\n",
        "                'total_results': total_results\n",
        "            }\n",
        "\n",
        "print(\"Processing complete for all folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wJeVygXnVPQS",
      "metadata": {
        "id": "wJeVygXnVPQS"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/AI Final/youtube_frames/Best meal weve ever had - 6'\n",
        "per_image_results, total_results = process_folder(folder_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "geospatial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}