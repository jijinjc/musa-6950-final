# MUSA-6950-final

## Introduction

This github repository will showcase my MUSA 6950, AI for Urban Sustainability Final Project. In this project I attempted to use MaskRCNN to identify images within a series of non-geographic YouTube videos to better understand urban and rural environments in Japan. The choice for pursuing this topic is because many YouTube videos are NOT recorded for identifying urban information. If there is a way to identify some trends between environments through these type of videos, it could possibly contribute to expanding a geographic data base for many people to use. 

This project will poke at this idea of obtaining useful information from data not meant to be geographic datasets by looking at the distribution of common urban environment objects throughout different regions of Japan. The project uses the YouTube playlist [tip-to-tip](https://www.youtube.com/watch?v=SHIkv0XH20A&list=PLLGT0cEMIAzeq_YFR_iHm831-GuOWlwUJ), created by YouTuber Ludwig, which documents him and YouTuber Michaell Reeves' travel from the Southern tip of Japan (Cape Sata) to the Northern tip of Japan (Cape Soya) using no maps and highways on their motorcycles. This playlist was chosen as it covers various locations of Japan through a means in which different landscapes are captured, albeit unintentionally. 

To do this, tools such as labelme and pytorch's MaskRCNN models are used to identify four main classes: Buildings, People, Non-Motorcycle Vehicles, and Motorcycles. By using MaskRCNN, we are able to identify these classes in different frames of a video, and count the occurence of these classes throughout the various different locations they pass. While the video shows some geographical information each video, they only show distance to the final locations. Based on this, we simply looked at the videos based on the episode of the playlist, and look at the change in the how much the model was able to identify certain classes within the urban environment. Because they move from Southwest to Northeast, we can have a simple understanding of how the occurence of these classes change as they move more closer to the Northeast.

## Methodology

This project revolves the idea of obtaining data that usually would not be considered proper geographic data, as therefore typically does not contain geographic information that is readily retrievable from these data sources. In ordder to combat this, several different codes were used to obtain data, wrangle data, and process data accordingly. In this section, there will be a brief overview of every code, the packages it uses, and the general workflow in these files in how they function and what they do. 

To start off, we will be looking at obtaining the data. Since this project uses YouTube videos, we will need to be able to find a reliable way to download these YouTube videos. As mentioned above in the introduction, we are using a specified playlist. Thus, we can look at [download-youtube-videos.py](https://github.com/jijinjc/musa-6950-final/blob/main/download_youtube_videos.py) as our main code to accomplish this task. This code relies on the yt_dlp package and the cv2 (opencv) packages to complete this process. The main inputs of this code are base_dir, and the url of the playlist you will be downloading from. Not only will the code download, there are also sections of the code that could help you retrieve random frames from the video, should you need it for testing or validation.

However, to ensure that our input dataset for training is the most optimal, we must filter the videos for sections that will be the most useful. Unfortunately, this project does not tackle this issue autonomously via code. Instead, videos were manually watched, and certain clips were chosen to be fitting. For this study, the clips that were chosen were clips that typically included at least one target class that we are labelling, but many clips included several. A majority of these clips were taken when the time of day were before dark, such that objects could be clearly identified. These clips were handpicked using ClipCut, a video editing software. Once these clips are obtained you can use [extract_all_frames.py](https://github.com/jijinjc/musa-6950-final/blob/main/extract_all_frames.py) which allows you to obtain frames from the clips at a target fram per second (fps). For this study, 1 fps was chosen for simplicity, and still yielded plenty of data. Another manual quality check is also needed, to make sure that any blurry images would be removed, or if there are too many similar frames. 

With the data properly obtained, we will need to label the images. To do this, we boot up the labelme software, and highlight the four classes that we have idenified. Then, we can use the Mask R-CNN model using the [You_Mask_RCNN.ipynb](https://github.com/jijinjc/musa-6950-final/blob/main/You_Mask_RCNN.ipynb) notebook. This notebook uses a simple ResNet-50 FPN V2 backbone from torchvision library to do the segmentation. The notebook will allow us to prepare training and testing data first, allowing us to test the annotation of images you label. The code then loads the Mask R-CNN model, trains the model, and fine-tunes the model. After the model has been properly trained and set up, the code visualizes basic results by comparing an input image and the model's identification of the classes. This visualization is not what we are looking at for our goal, as we are trying to see how the urban environment changes. 

Therefore, we can also use the [extract_video_frames.py](https://github.com/jijinjc/musa-6950-final/blob/main/extract_video_frames.py) code to get a frame from each of the 14 videos in the play list every 10 seconds. This allows us to get a relatively fair gauge of the urban environment that are displayed in the videos. With the frames of the 14 videos all compiled, we can then feed it back to the Mask R-CNN notebook at the end to calculate the occurrences of each image class per episode at the end of the document, and save them to a csv.

With the csvs, we can finally use the [visualization.ipynb](https://github.com/jijinjc/musa-6950-final/blob/main/visualization.ipynb) to complete the visualization process, and have a sense of how the urban environment changes.

## Results



