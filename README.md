# MUSA-6950-final

## Introduction

This github repository will showcase my MUSA 6950, AI for Urban Sustainability Final Project. In this project I attempted to use MaskRCNN to identify images within a series of non-geographic YouTube videos to better understand urban and rural environments in Japan. The choice for pursuing this topic is because many YouTube videos are NOT recorded for identifying urban information. If there is a way to identify some trends between environments through these type of videos, it could possibly contribute to expanding a geographic data base for many people to use. 

This project will poke at this idea of obtaining useful information from data not meant to be geographic datasets by looking at the distribution of common urban environment objects throughout different regions of Japan. The project uses the YouTube playlist [tip-to-tip](https://www.youtube.com/watch?v=SHIkv0XH20A&list=PLLGT0cEMIAzeq_YFR_iHm831-GuOWlwUJ), created by YouTuber Ludwig, which documents him and YouTuber Michaell Reeves' travel from the Southern tip of Japan (Cape Sata) to the Northern tip of Japan (Cape Soya) using no maps and highways on their motorcycles. This playlist was chosen as it covers various locations of Japan through a means in which different landscapes are captured, albeit unintentionally. 

To do this, tools such as labelme and pytorch's MaskRCNN models are used to identify four main classes: Buildings, People, Non-Motorcycle Vehicles, and Motorcycles. By using MaskRCNN, we are able to identify these classes in different frames of a video, and count the occurence of these classes throughout the various different locations they pass. While the video shows some geographical information each video, they only show distance to the final locations. Based on this, we simply looked at the videos based on the episode of the playlist, and look at the change in the how much the model was able to identify certain classes within the urban environment. Because they move from Southwest to Northeast, we can have a simple understanding of how the occurence of these classes change as they move more closer to the Northeast.

## Methodology

This project revolves the idea of obtaining data that usually would not be considered proper geographic data, as therefore typically does not contain geographic information that is readily retrievable from these data sources. In ordder to combat this, several different codes were used to obtain data, wrangle data, and process data accordingly. In this section, there will be a brief overview of every code, the packages it uses, and the general workflow in these files in how they function and what they do. 

To start off, we will be looking at obtaining the data. Since this project uses YouTube videos, we will need to be able to find a reliable way to download these YouTube videos. As mentioned above in the introduction, we are using a specified playlist. Thus, we can look at [download-youtube-videos.py](https://github.com/jijinjc/musa-6950-final/blob/main/download_youtube_videos.py) as our main code to accomplish this task. This code relies on the yt_dlp package and the cv2 (opencv) packages to complete this process. The main inputs of this code are base_dir, and the url of the playlist you will be downloading from. Not only will the code download, there are also sections of the code that could help you retrieve random frames from the video, should you need it for testing or validation.

However, to ensure that our input dataset for training is the most optimal, we must filter the videos for sections that will be the most useful. Unfortunately, this project does not tackle this issue autonomously via code. Instead, videos were manually watched, and certain clips were chosen to be fitting. For this study, the clips that were chosen were clips that typically included at least one target class that we are labelling, but many clips included several. A majority of these clips were taken when the time of day were before dark, such that objects could be clearly identified. These clips were handpicked using ClipCut, a video editing software. Once these clips are obtained you can use [extract_all_frames.py](https://github.com/jijinjc/musa-6950-final/blob/main/extract_all_frames.py) which allows you to obtain frames from the clips at a target fram per second (fps). For this study, 1 fps was chosen for simplicity, and still yielded plenty of data. Another manual quality check is also needed, to make sure that any blurry images would be removed, or if there are too many similar frames. 

With the data properly obtained, we will need to label the images. To do this, we boot up the labelme software, and highlight the four classes that we have idenified. Then, we can use the Mask R-CNN model using the [You_Mask_RCNN.ipynb](https://github.com/jijinjc/musa-6950-final/blob/main/You_Mask_RCNN.ipynb) notebook. This notebook uses a simple ResNet-50 FPN V2 backbone from torchvision library to do the segmentation. The notebook will allow us to prepare training and testing data first, allowing us to test the annotation of images you label. The code then loads the Mask R-CNN model, trains the model, and fine-tunes the model. After the model has been properly trained and set up, the code visualizes basic results by comparing an input image and the model's identification of the classes. This visualization is not what we are looking at for our goal, as we are trying to see how the urban environment changes. 

Therefore, we can also use the [extract_video_frames.py](https://github.com/jijinjc/musa-6950-final/blob/main/extract_video_frames.py) code to get a frame from each of the 14 videos in the play list every 10 seconds. This allows us to get a relatively fair gauge of the urban environment that are displayed in the videos. With the frames of the 14 videos all compiled, we can then feed it back to the Mask R-CNN notebook at the end to calculate the occurrences of each image class per episode at the end of the document, and save them to a csv.

With the csvs, we can finally use the [visualization.ipynb](https://github.com/jijinjc/musa-6950-final/blob/main/visualization.ipynb) to complete the visualization process, and have a sense of how the urban environment changes.

## Results & Discussion

![Image](https://github.com/user-attachments/assets/4c30019b-6ff0-4d07-a5ee-2baba7649bf2)

Based on our this example provided, it is clear that there are several benefits and limitations of our current model. While we can see that generally the model was able to identify objects quite accurately, there are still some errors when defining some of the classes. This likely means that changes still need to be made to the model. However, for this level of accuracy, we can look at employing in a rudimentary analysis, especially for what we are doing in observing how the urban environment changes.

![Image](https://github.com/user-attachments/assets/41ecfa1a-20cf-4745-ab5c-4aec7d2ef4c2)

From this simple result we can get a simple understanding of how the urban environment composition changed as their distance to the final location Cape Soya in Hokkaido has changed. These values are recorded based on the distance they are to Cape Soya at the end of the day, which is why there an increase from 1514 to 1516km. We can see that generally the building count was the highest in Japan throughout their journey, except for their second day, which saw a lot of cars. Unfortunately, these results do not paint much of a picture, except that we could maybe infer that at 627 km (Day 10) they journeyed past a large city that showed an increase of all classes, or that during the second day they were on the road for a majority of time, which could have been why there were more identified cars than other objects. 

For discussion purposes, I believe that it is important to note several improvements could be made. First of all with the model and the dataset, we could use a stronger more advanced model, such as ResNet 101, or ResNeXt, as well as increase the dataset size. These could make the model stronger in identifying classes more correctly, and with higher accuracy. Additionally, the results could have been greatly affected by the type of datasets that was used. Because the dataset came from videos that was focused more on a journey across the country, it was more difficult to identify urban environments. It may be interesting in the future to focus on a few major cities, and use much more images to create a dataset and eventually help identify which city the images come from, similar to the idea of a reverse 'geoguessr' project.

## Conclusion

All in all, this was a very interesting project to work on. Through this project, we were able to observe how well a model is able to take an input data from a series of videos, and apply it to other frames in the series. While the model was successful in identifying many of the classes that were labelled, it may be more difficult for us to infer proper data from these data, especially due to the fact that there is a lack of strong connection between the images from the videos and a real life location. While we were able to use broader locations, it was hard to pinpoint exact locations. In the future, the study could benefit from focusing on the connection between images and real-life counterparts, which would make a huge step in being able to use data sources that would otherwise not be feasible as proper data.


